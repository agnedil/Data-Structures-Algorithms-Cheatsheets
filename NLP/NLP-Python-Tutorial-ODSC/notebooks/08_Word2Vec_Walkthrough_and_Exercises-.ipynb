{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "<tr>\n",
    "\n",
    "<th, style=\"background-color:white\">\n",
    "<img src=\"https://github.com/mlgill/ODSC_East_2017_PythonNLP/blob/master/assets/logo.png?raw=true\", width=140, height=100>\n",
    "</th>\n",
    "\n",
    "<th, style=\"background-color:white\">\n",
    "<div align=\"left\">\n",
    "<h1>Learning from Text: <br> Introduction to Natural Language Processing with Python</h1>  \n",
    "<h2>Michelle L. Gill, Ph.D.</h2>     \n",
    "Senior Data Scientist, Metis  \n",
    "ODSC East  \n",
    "May 3, 2017 \n",
    "</div>\n",
    "</th>\n",
    "\n",
    "</tr>\n",
    "</table>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Walkthrough and Exercises\n",
    "\n",
    "Begin by loading Google's pre-trained Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:48:59.886618Z",
     "start_time": "2017-05-03T11:48:57.705971Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "from accessory_functions import google_vec_file, nltk_path\n",
    "\n",
    "# Setup nltk corpora path\n",
    "nltk.data.path.insert(0, nltk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:51:40.775431Z",
     "start_time": "2017-05-03T11:49:00.188360Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format(google_vec_file, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google's model contains an extensive vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:39:17.118183Z",
     "start_time": "2017-05-03T11:39:17.105745Z"
    }
   },
   "outputs": [],
   "source": [
    "type(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:39:17.125725Z",
     "start_time": "2017-05-03T11:39:17.120014Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_list = model.vocab.keys()\n",
    "len(vocab_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary Features\n",
    "\n",
    "Each word contains an array of 300 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:39:17.139724Z",
     "start_time": "2017-05-03T11:39:17.134280Z"
    }
   },
   "outputs": [],
   "source": [
    "len(model.word_vec('cat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:39:17.155341Z",
     "start_time": "2017-05-03T11:39:17.146501Z"
    }
   },
   "outputs": [],
   "source": [
    "model.word_vec('cat')[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cosine similarity between words can be computed and produces intuitive trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:39:17.166023Z",
     "start_time": "2017-05-03T11:39:17.157393Z"
    }
   },
   "outputs": [],
   "source": [
    "print(model.similarity('cat', 'cat'))\n",
    "print(model.similarity('cat', 'dog'))\n",
    "print(model.similarity('cat', 'car'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:39:17.177706Z",
     "start_time": "2017-05-03T11:39:17.167922Z"
    }
   },
   "outputs": [],
   "source": [
    "print(model.similarity('car', 'truck'))\n",
    "print(model.similarity('car', 'drive'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec captures some interesting similarities between words, such as the relationship between **man --> king** and **woman --> queen**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:39:35.021107Z",
     "start_time": "2017-05-03T11:39:17.181872Z"
    }
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can also detect words that don't belong in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:39:35.031158Z",
     "start_time": "2017-05-03T11:39:35.023312Z"
    }
   },
   "outputs": [],
   "source": [
    "model.doesnt_match(\"breakfast cereal dinner lunch\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Word2Vec in Models\n",
    "\n",
    "Let's load the spam/ham classification data set, split it into train/test sets, and add Word2Vec features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:39:41.269855Z",
     "start_time": "2017-05-03T11:39:35.032704Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from accessory_functions import preprocess_series_text, nltk_path\n",
    "\n",
    "data = pd.read_csv('../data/spam.csv', sep='\\t')\n",
    "data['text'] = preprocess_series_text(data.text, nltk_path=nltk_path)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:39:41.280957Z",
     "start_time": "2017-05-03T11:39:41.271557Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.text\n",
    "y = data.label\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                test_size=0.3,\n",
    "                                random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:39:41.381298Z",
     "start_time": "2017-05-03T11:39:41.282905Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "X_test_cv  = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the Word2Vec vector for each word in the vocabulary. Store in a dictionary for faster retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:39:41.394809Z",
     "start_time": "2017-05-03T11:39:41.382865Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_list = cv.get_feature_names()\n",
    "len(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:39:41.714128Z",
     "start_time": "2017-05-03T11:39:41.396634Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_dict = dict([(x, model.word_vec(x)) for x in feature_list\n",
    "                      if x in vocab_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each document, get an average of the vector mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:39:42.451389Z",
     "start_time": "2017-05-03T11:39:41.715490Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "def embed_words(document, feature_dict):\n",
    "    # get a list of all words for which there is a vector embedding\n",
    "    feature_list = feature_dict.keys()\n",
    "    \n",
    "    # split the document into words\n",
    "    words = word_tokenize(document)\n",
    "    \n",
    "    # store all vector embeddings for a document\n",
    "    vector_list = list()\n",
    "    for w in words:\n",
    "        if w in feature_list:\n",
    "            vector_list.append(feature_dict[w])\n",
    "    \n",
    "    # return mean value of vector embeddings\n",
    "    if len(vector_list) > 0:\n",
    "        vector = np.mean(vector_list, axis=0)\n",
    "    else:\n",
    "        vector = np.zeros(300)\n",
    "        \n",
    "    return vector\n",
    "\n",
    "\n",
    "# create vector embeddings\n",
    "X_train_embed = X_train.apply(lambda x: embed_words(x, feature_dict))\n",
    "X_test_embed  = np.array(X_test.apply(lambda x: embed_words(x, feature_dict)))\n",
    "\n",
    "# force into two-dimensional numpy array\n",
    "X_train_embed = np.array([x for x in X_train_embed])\n",
    "X_test_embed = np.array([x for x in X_test_embed])\n",
    "\n",
    "print(X_train_embed.shape, X_test_embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These embeddings can be used in a machine learning classifier alone or joined to a document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:39:42.793494Z",
     "start_time": "2017-05-03T11:39:42.453206Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine the document-term matrix and the word2vec embeddings\n",
    "\n",
    "X_train_comb = np.hstack((X_train_cv.toarray(), X_train_embed))\n",
    "X_test_comb = np.hstack((X_test_cv.toarray(), X_test_embed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "* Using Logistic regression, fit models using the three feature matrices: document-term alone, word2vec embeddings alone, and both combined.\n",
    "* Compare the accuracy of each of these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
