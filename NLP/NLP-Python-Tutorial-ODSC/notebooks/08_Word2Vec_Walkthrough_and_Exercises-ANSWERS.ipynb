{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "<tr>\n",
    "\n",
    "<th, style=\"background-color:white\">\n",
    "<img src=\"https://github.com/mlgill/ODSC_East_2017_PythonNLP/blob/master/assets/logo.png?raw=true\", width=140, height=100>\n",
    "</th>\n",
    "\n",
    "<th, style=\"background-color:white\">\n",
    "<div align=\"left\">\n",
    "<h1>Learning from Text: <br> Introduction to Natural Language Processing with Python</h1>  \n",
    "<h2>Michelle L. Gill, Ph.D.</h2>     \n",
    "Senior Data Scientist, Metis  \n",
    "ODSC East  \n",
    "May 3, 2017 \n",
    "</div>\n",
    "</th>\n",
    "\n",
    "</tr>\n",
    "</table>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Walkthrough and Exercise Answers\n",
    "\n",
    "Begin by loading Google's pre-trained Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:37:11.442616Z",
     "start_time": "2017-05-03T11:37:10.059833Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "from accessory_functions import google_vec_file, nltk_path\n",
    "\n",
    "# Setup nltk corpora path\n",
    "nltk.data.path.insert(0, nltk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:39:17.103905Z",
     "start_time": "2017-05-03T11:37:11.443998Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format(google_vec_file, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google's model contains an extensive vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:39:17.118183Z",
     "start_time": "2017-05-03T11:39:17.105745Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:39:17.125725Z",
     "start_time": "2017-05-03T11:39:17.120014Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list = model.vocab.keys()\n",
    "len(vocab_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary Features\n",
    "\n",
    "Each word contains an array of 300 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:39:17.139724Z",
     "start_time": "2017-05-03T11:39:17.134280Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.word_vec('cat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:39:17.155341Z",
     "start_time": "2017-05-03T11:39:17.146501Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0123291 ,  0.20410156, -0.28515625,  0.21679688,  0.11816406,\n",
       "        0.08300781,  0.04980469, -0.00952148,  0.22070312, -0.12597656,\n",
       "        0.08056641, -0.5859375 , -0.00445557, -0.296875  , -0.01312256,\n",
       "       -0.08349609,  0.05053711,  0.15136719, -0.44921875, -0.0135498 ], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.word_vec('cat')[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cosine similarity between words can be computed and produces intuitive trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:39:17.166023Z",
     "start_time": "2017-05-03T11:39:17.157393Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.760945708978\n",
      "0.215281850364\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('cat', 'cat'))\n",
    "print(model.similarity('cat', 'dog'))\n",
    "print(model.similarity('cat', 'car'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:39:17.177706Z",
     "start_time": "2017-05-03T11:39:17.167922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.673579016963\n",
      "0.313944691624\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('car', 'truck'))\n",
    "print(model.similarity('car', 'drive'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec captures some interesting similarities between words, such as the relationship between **man --> king** and **woman --> queen**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:39:35.021107Z",
     "start_time": "2017-05-03T11:39:17.181872Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192911148071),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431607246399)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can also detect words that don't belong in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:39:35.031158Z",
     "start_time": "2017-05-03T11:39:35.023312Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cereal'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"breakfast cereal dinner lunch\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Word2Vec in Models\n",
    "\n",
    "Let's load the spam/ham classification data set, split it into train/test sets, and add Word2Vec features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:39:41.269855Z",
     "start_time": "2017-05-03T11:39:35.032704Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>early bird purchase yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>hi mandy sullivan call hotmix fm choose receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>heart empty without love mind empty without wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>yes start send request make pain come back bac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>see swing bit get thing take care firsg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham                            early bird purchase yet\n",
       "1  spam  hi mandy sullivan call hotmix fm choose receiv...\n",
       "2   ham  heart empty without love mind empty without wi...\n",
       "3   ham  yes start send request make pain come back bac...\n",
       "4   ham            see swing bit get thing take care firsg"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from accessory_functions import preprocess_series_text, nltk_path\n",
    "\n",
    "data = pd.read_csv('../data/spam.csv', sep='\\t')\n",
    "data['text'] = preprocess_series_text(data.text, nltk_path=nltk_path)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:39:41.280957Z",
     "start_time": "2017-05-03T11:39:41.271557Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.text\n",
    "y = data.label\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                test_size=0.3,\n",
    "                                random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:39:41.381298Z",
     "start_time": "2017-05-03T11:39:41.282905Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "X_test_cv  = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the Word2Vec vector for each word in the vocabulary. Store in a dictionary for faster retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:39:41.394809Z",
     "start_time": "2017-05-03T11:39:41.382865Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5482"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_list = cv.get_feature_names()\n",
    "len(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:39:41.714128Z",
     "start_time": "2017-05-03T11:39:41.396634Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_dict = dict([(x, model.word_vec(x)) for x in feature_list\n",
    "                      if x in vocab_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each document, get an average of the vector mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:39:42.451389Z",
     "start_time": "2017-05-03T11:39:41.715490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3900, 300) (1672, 300)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "def embed_words(document, feature_dict):\n",
    "    # get a list of all words for which there is a vector embedding\n",
    "    feature_list = feature_dict.keys()\n",
    "    \n",
    "    # split the document into words\n",
    "    words = word_tokenize(document)\n",
    "    \n",
    "    # store all vector embeddings for a document\n",
    "    vector_list = list()\n",
    "    for w in words:\n",
    "        if w in feature_list:\n",
    "            vector_list.append(feature_dict[w])\n",
    "    \n",
    "    # return mean value of vector embeddings\n",
    "    if len(vector_list) > 0:\n",
    "        vector = np.mean(vector_list, axis=0)\n",
    "    else:\n",
    "        vector = np.zeros(300)\n",
    "        \n",
    "    return vector\n",
    "\n",
    "\n",
    "# create vector embeddings\n",
    "X_train_embed = X_train.apply(lambda x: embed_words(x, feature_dict))\n",
    "X_test_embed  = np.array(X_test.apply(lambda x: embed_words(x, feature_dict)))\n",
    "\n",
    "# force into two-dimensional numpy array\n",
    "X_train_embed = np.array([x for x in X_train_embed])\n",
    "X_test_embed = np.array([x for x in X_test_embed])\n",
    "\n",
    "print(X_train_embed.shape, X_test_embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These embeddings can be used in a machine learning classifier alone or joined to a document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:39:42.793494Z",
     "start_time": "2017-05-03T11:39:42.453206Z"
    }
   },
   "outputs": [],
   "source": [
    "# combine the document-term matrix and the word2vec embeddings\n",
    "\n",
    "X_train_comb = np.hstack((X_train_cv.toarray(), X_train_embed))\n",
    "X_test_comb = np.hstack((X_test_cv.toarray(), X_test_embed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "* Using Logistic regression, fit models using the three feature matrices: document-term alone, word2vec embeddings alone, and both combined.\n",
    "* Compare the accuracy of each of these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:39:53.867851Z",
     "start_time": "2017-05-03T11:39:42.795809Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "lr = LogisticRegressionCV()\n",
    "\n",
    "# document-term alone\n",
    "lr.fit(X_train_cv, y_train)\n",
    "y_pred_cv = lr.predict(X_test_cv)\n",
    "\n",
    "# word2vec embeddings\n",
    "lr.fit(X_train_embed, y_train)\n",
    "y_pred_embed = lr.predict(X_test_embed)\n",
    "\n",
    "# combined\n",
    "lr.fit(X_train_comb, y_train)\n",
    "y_pred_comb = lr.predict(X_test_comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the error metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:39:53.883410Z",
     "start_time": "2017-05-03T11:39:53.869726Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# store each accuracy in a list\n",
    "accuracy_list = list()\n",
    "\n",
    "for lab,pred in zip(['document-term', 'word2vec', 'combined'],\n",
    "                    [y_pred_cv, y_pred_embed, y_pred_comb]):\n",
    "    accuracy_list.append((lab, accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a small improvement in this data set with the combined set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T11:39:53.900393Z",
     "start_time": "2017-05-03T11:39:53.885453Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>document-term</td>\n",
       "      <td>0.979665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>0.958134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>combined</td>\n",
       "      <td>0.983254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model  accuracy\n",
       "0  document-term  0.979665\n",
       "1       word2vec  0.958134\n",
       "2       combined  0.983254"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(accuracy_list,\n",
    "             columns=['model', 'accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
