{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "<tr>\n",
    "\n",
    "<th, style=\"background-color:white\">\n",
    "<img src=\"https://github.com/mlgill/ODSC_East_2017_PythonNLP/blob/master/assets/logo.png?raw=true\", width=140, height=100>\n",
    "</th>\n",
    "\n",
    "<th, style=\"background-color:white\">\n",
    "<div align=\"left\">\n",
    "<h1>Learning from Text: <br> Introduction to Natural Language Processing with Python</h1>  \n",
    "<h2>Michelle L. Gill, Ph.D.</h2>     \n",
    "Senior Data Scientist, Metis  \n",
    "ODSC East  \n",
    "May 3, 2017 \n",
    "</div>\n",
    "</th>\n",
    "\n",
    "</tr>\n",
    "</table>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Text Clustering Exercises\n",
    "\n",
    "We will be using a subset of the Reuters data set, which is a collection of 9,603 newswire articles. The training set contains training articles from April 7, 1987 and a test set from the following day (April 8, 1987).\n",
    "\n",
    "This dataset is included with the NLTK corpora, so the initial code will handle loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T10:18:49.148622Z",
     "start_time": "2017-05-03T10:18:47.866462Z"
    },
    "collapsed": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "sns.set_style('white')\n",
    "sns.set_palette('dark')\n",
    "\n",
    "import nltk\n",
    "from accessory_functions import nltk_path\n",
    "# Setup nltk corpora path\n",
    "nltk.data.path.insert(0, nltk_path)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T10:18:49.476911Z",
     "start_time": "2017-05-03T10:18:49.150776Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "\n",
    "reuters.ensure_loaded()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to load the data and create data and category dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T10:18:49.494425Z",
     "start_time": "2017-05-03T10:18:49.478475Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(data_obj):\n",
    "    \n",
    "    # Sort the filenames into train and test\n",
    "    category_docs = data_obj.fileids()\n",
    "\n",
    "    # Get the text for the train and test files\n",
    "    text = [data_obj.raw(x) for x in category_docs]\n",
    "\n",
    "    # Create dataframe\n",
    "    data_df = pd.DataFrame({'fileid':category_docs, \n",
    "                             'text':text}).set_index('fileid')\n",
    "\n",
    "    # Load the categories and create a dataframe\n",
    "    category_list = data_obj.categories()\n",
    "\n",
    "    category_dict = [(pd.DataFrame({x: data_obj.fileids(x)})\n",
    "                      .stack()\n",
    "                      .to_frame()\n",
    "                      .reset_index(level=-1))\n",
    "                     for x in category_list]\n",
    "    category_df = pd.concat(category_dict, axis=0).reset_index(drop=True)\n",
    "    category_df.columns = ['category', 'fileid']\n",
    "    category_df = category_df.set_index('category')\n",
    "    \n",
    "    return data_df, category_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T10:18:52.064254Z",
     "start_time": "2017-05-03T10:18:49.497294Z"
    }
   },
   "outputs": [],
   "source": [
    "data, category = load_data(reuters)\n",
    "\n",
    "print(data.shape, category.shape)\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select just the cocoa and coffee articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T10:18:52.083500Z",
     "start_time": "2017-05-03T10:18:52.066126Z"
    }
   },
   "outputs": [],
   "source": [
    "cocoa = category.loc['cocoa']\n",
    "coffee = category.loc['coffee']\n",
    "\n",
    "cocoa_data = data.loc[cocoa.fileid]\n",
    "coffee_data = data.loc[coffee.fileid]\n",
    "\n",
    "data_sm = pd.concat([cocoa_data, coffee_data])\n",
    "\n",
    "print(data_sm.shape, data_sm.drop_duplicates().shape)\n",
    "\n",
    "data_sm = data_sm.drop_duplicates().sample(frac=1, replace=False,\n",
    "                                           random_state=42)\n",
    "\n",
    "data = data_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you prefer to work with lists rather than Pandas dataframes, they can be created by copying and pasting the following code into a cell. This should be executed after preprocessing the data in Question 1 below.\n",
    "\n",
    "```python\n",
    "data = data.text.tolist()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "* Preprocess the data using the convenience function in `accessory_functions`\n",
    "* Use count vectorizer to create a document-term matrix with counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "* Perform clustering on a few of the documents using either cosine distance (1 - cosine_similarity) or Euclidean distance.\n",
    "* Plot the results in a dendrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "* Get the flattened clusters for one of the linkage matrices using `fcluster`\n",
    "* Using the cluster number, print some of each of the articles in the respective clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
