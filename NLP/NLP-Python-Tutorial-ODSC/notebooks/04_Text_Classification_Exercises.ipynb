{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "<tr>\n",
    "\n",
    "<th, style=\"background-color:white\">\n",
    "<img src=\"https://github.com/mlgill/ODSC_East_2017_PythonNLP/blob/master/assets/logo.png?raw=true\", width=140, height=100>\n",
    "</th>\n",
    "\n",
    "<th, style=\"background-color:white\">\n",
    "<div align=\"left\">\n",
    "<h1>Learning from Text: <br> Introduction to Natural Language Processing with Python</h1>  \n",
    "<h2>Michelle L. Gill, Ph.D.</h2>     \n",
    "Senior Data Scientist, Metis  \n",
    "ODSC East  \n",
    "May 3, 2017 \n",
    "</div>\n",
    "</th>\n",
    "\n",
    "</tr>\n",
    "</table>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification Exercises\n",
    "\n",
    "We will be using the SMS spam data from the previous set of exercises. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T10:16:25.268422Z",
     "start_time": "2017-05-03T10:16:24.200097Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('white')\n",
    "sns.set_palette('dark')\n",
    "sns.set_context('talk')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and, for convenience, use the preprocessing function from `accessory_functions`. Alternatively, the pickled preprocessed dataframe could be loaded with the following code:\n",
    "\n",
    "```python\n",
    "data = pd.read_pickle('../data/spam_preprocessed.pkl')\n",
    "data.head()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T10:16:33.315844Z",
     "start_time": "2017-05-03T10:16:25.271005Z"
    }
   },
   "outputs": [],
   "source": [
    "from accessory_functions import preprocess_series_text, nltk_path\n",
    "\n",
    "data = pd.read_csv('../data/spam.csv', sep='\\t')\n",
    "data['text'] = preprocess_series_text(data.text, nltk_path=nltk_path)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into X (features) and y (predictor) data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T10:16:33.321130Z",
     "start_time": "2017-05-03T10:16:33.317544Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data.text\n",
    "y = data.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "* Split in to train and test sets\n",
    "* Create numerical features with Count Vectorizer. Multiple sets of features can be created with binary or integer word counts and either unigram (single word) or multi-gram features. Multiple sets of features can be created for comparison, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "* Create and train a logistic regression model using the count vectorizer results\n",
    "* Measure the error on these models and compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 3\n",
    "\n",
    "Pick any other machine learning model and train it. Naive Bayes is an easy one because it doesn't have many additional parameters to tune. For count data, use [Multinomial Naive Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB). For binary data, choose [Bernoulli Naive Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "**NOTE:** this question should be completed after the TF-IDF section is discussed.\n",
    "\n",
    "Use Scikit-learn's [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) to create a term document matrix and then train one of the models used previously. The syntax is similar to Count Vectorizer. TF-IDF has parameters similar to those of Count Vectorizer. However, `use_binary` is not a feature, but `use_idf` is.\n",
    "\n",
    "Here is an example:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
